
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Responsible AI in the Age of Generative AI &#8212; Responsible AI for Data Scientists</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/4-1-GenAI';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Resources" href="resources.html" />
    <link rel="prev" title="Privacy Mechanisms" href="3-2-mechanism.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="0-1-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.webp" class="logo__image only-light" alt="Responsible AI for Data Scientists - Home"/>
    <script>document.write(`<img src="../_static/logo.webp" class="logo__image only-dark" alt="Responsible AI for Data Scientists - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="0-1-intro.html">
                    Responsible AI for Data Scientists
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0-2-framework.html">A Human-Centric Framework of Responsible AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="0-3-philosophy.html">Philosophical Foundations for Ethical Decision-Making</a></li>
<li class="toctree-l1"><a class="reference internal" href="0-4-prep.html">Fundementals of Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Fairness and Algorithmic Bias</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1-1-intro.html">Introduction to Algorithmic Bias and Fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="1-2-measure.html">Measuring Fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="1-3-mitigation.html">Mitigating Algorithmic Bias</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Transparency and Interpretable Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2-1-intro.html">Introduction to Interpretable Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="2-2-global.html">Methods for Global Interpretations</a></li>
<li class="toctree-l1"><a class="reference internal" href="2-3-local.html">Methods for Local Interpretations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="3-1-framework.html">Conceptual Framework of Privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="3-2-mechanism.html">Privacy Mechanisms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Special Topics</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Responsible AI in the Age of Generative AI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="resources.html">Resources</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="zbib.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/mochenyang/Responsible-AI" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/content/4-1-GenAI.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Responsible AI in the Age of Generative AI</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#same-problems-new-challenges">Same Problems, New Challenges</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#genai-and-fairness">GenAI and Fairness</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#genai-and-interpretability">GenAI and Interpretability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#genai-privacy-and-security">GenAI, Privacy, and Security</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-challenge-ai-alignment">New Challenge: AI Alignment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-very-brief-introduction-to-reinforcement-learning">A Very Brief Introduction to Reinforcement Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-alignment-hard">Why Is Alignment Hard?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-challenge-robustness">New Challenge: Robustness</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="responsible-ai-in-the-age-of-generative-ai">
<h1>Responsible AI in the Age of Generative AI<a class="headerlink" href="#responsible-ai-in-the-age-of-generative-ai" title="Link to this heading">#</a></h1>
<p>Generative AI (GenAI) systems, such as large language models, represent arguably one of the most exciting recent advances in the field of AI. The same responsible AI issues, including fairness, interpretability, and privacy, remain highly relevant (if not more so) for GenAI, and they sometimes take on new forms. Meanwhile, GenAI also presents a number of emerging challenges that are worth discussing. This chapter provides an exposition of responsible AI in GenAI systems.</p>
<section id="same-problems-new-challenges">
<h2>Same Problems, New Challenges<a class="headerlink" href="#same-problems-new-challenges" title="Link to this heading">#</a></h2>
<section id="genai-and-fairness">
<h3>GenAI and Fairness<a class="headerlink" href="#genai-and-fairness" title="Link to this heading">#</a></h3>
<p>GenAI systems have been shown to generate texts / images with biases against certain protected groups <span id="id1">[<a class="reference internal" href="zbib.html#id280" title="Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, and Jun Xu. Bias and unfairness in information retrieval systems: new challenges in the llm era. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 6437–6447. 2024.">DXX+24</a>]</span>. For instance, LLM-generated item recommendations exhibit stereotyping <span id="id2">[<a class="reference internal" href="zbib.html#id281" title="Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, 993–999. 2023.">ZBZ+23</a>]</span>, image-generation models such as DALL-E show gender and racial bias (e.g., generating male/female-dominant images for occupations like pilot/receptionist <span id="id3">[<a class="reference internal" href="zbib.html#id282" title="Marc Cheong, Ehsan Abedin, Marinus Ferreira, Ritsaart Reimann, Shalom Chalson, Pamela Robinson, Joanne Byrne, Leah Ruppanner, Mark Alfano, and Colin Klein. Investigating gender and racial biases in dall-e mini images. ACM Journal on Responsible Computing, 1(2):1–20, 2024.">CAF+24</a>]</span>), and LLMs perpetuate existing biases in chat responses (e.g., associating males with higher education levels, <span id="id4">[<a class="reference internal" href="zbib.html#id283" title="Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language models. arXiv preprint arXiv:2308.10149, 2023.">LDS+23</a>]</span>).</p>
<p>Biases in GenAI can be attributed to both the training data and training method of these systems (corresponding to the Data and Model components in our Responsible AI framework). GenAI systems are <em>pre-trained</em> on massive, Internet-scale raw data, which are polluted by existing biases. Moreover, to achieve a level of “general intelligence”, GenAI systems are largely trained to achieve desirable accuracy on the next-token completion task, without fairness explicitly being a part of the training objective.</p>
<p>Mitigating biases in GenAI systems presents several new challenges that are qualitatively different from those that are discussed in Chapter <a class="reference internal" href="1-3-mitigation.html"><span class="doc">Mitigating Algorithmic Bias</span></a>.</p>
<p>First, the scale and cost of GenAI pre-training substantially constrain data-level interventions. As mentioned before, GenAI systems are pre-trained on Internet-scale data consisting of heterogeneous and largely uncurated sources. At this scale, systematically identifying, auditing, and correcting biases in the training data would be technically and economically infeasible. While it is possible to fine-tune a pre-trained model on a smaller, more carefully curated dataset to improve fairness-related behaviors, such post-hoc interventions may degrade the model’s general-purpose capabilities or introduce overcorrections that impair performance on downstream tasks. This tension is analogous to the fairness-accuracy tradeoff previously discussed in Chapter <a class="reference internal" href="1-1-intro.html"><span class="doc">Introduction to Algorithmic Bias and Fairness</span></a>.</p>
<p>Second, the opacity and complexity of GenAI models (more on this later) make model-level fairness interventions highly non-trivial. GenAI systems are trained through complex, multi-stage pipelines involving pre-training, instruction tuning, and alignment. Unlike standard machine learning models, where fairness constraints can sometimes be explicitly incorporated into the optimization objective, modifying the training objective of a GenAI model to directly account for fairness is far from obvious. This lack of transparency at the Model component complicates both diagnosis and remediation of bias.</p>
<p>Third, post-training alignment efforts introduce additional tensions. Techniques such as Reinforcement Learning from Human Feedback (RLHF, <span id="id5">[<a class="reference internal" href="zbib.html#id299" title="Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and others. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744, 2022.">OWJ+22</a>]</span>) rely on human judgments to shape model behavior along multiple dimensions, including (but not limited to) usefulness, harmlessness, truthfulness, and fairness. However, human preferences are neither uniform nor mutually consistent. Different human annotator populations may hold divergent views on what constitutes fair or appropriate model outputs. As a result, post-training alignment necessarily involves balancing a multitude of objectives rather than only optimizing for fairness, while the relationships between fairness and other alignment objectives are complicated and not well understood.</p>
</section>
<section id="genai-and-interpretability">
<h3>GenAI and Interpretability<a class="headerlink" href="#genai-and-interpretability" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>People outside the field are often surprised and alarmed to learn that we do not understand how our own AI creations work.  – Dario Amodei, <a class="reference external" href="https://www.darioamodei.com/post/the-urgency-of-interpretability">The Urgency of Interpretability</a></p>
</div></blockquote>
<p>It is exceedingly hard to explain why and how GenAI systems “do the things they do”. While transparency of complex ML / AL systems is not a new challenge (see Chapter <a class="reference internal" href="2-1-intro.html"><span class="doc">Introduction to Interpretable Machine Learning</span></a>), the issue is significantly magnified in the context of GenAI. The lack of transparency in these systems arises from multiple interacting factors, including:</p>
<p><strong>Model size</strong>. State-of-the-art GenAI systems are extremely large neural networks comprising billions to trillions of parameters. For example, GPT-3 (one of the earliest LLMs) contain on the order of 175 billion parameters <span id="id6">[<a class="reference internal" href="zbib.html#id285" title="Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and others. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.">BMR+20</a>]</span>, and frontier models (as of late 2025) such as Kimi-2 are reported to have more than a trillion parameters <span id="id7">[<a class="reference internal" href="zbib.html#id284" title="Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, and others. Kimi k2: open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025.">TBB+25</a>]</span>. The sheer dimensionality of these models makes it difficult to meaningfully relate internal representations to human-understandable concepts.</p>
<p><strong>Commercial pressures</strong>. Most leading GenAI systems are developed by private firms operating in an intensive competition landscape. As a result, the details of pre-training datasets, data processing procedures, model architectures, and training pipelines are often proprietary. This lack of disclosure limits external scrutiny and independent auditing. It is worth noting that even LLMs labeled as “open source” generally only share the model weights but not necessarily details of the training data and training procedure.</p>
<p><strong>Emergent capabilities that are not well understood</strong>. Many behaviors observed in GenAI systems, such as in-context learning and chain-of-thought reasoning <span id="id8">[<a class="reference internal" href="zbib.html#id286" title="Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and others. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022.">WWS+22</a>]</span>, are not explicitly “built-in” but instead emerge as a byproduct of scale and training dynamics. Prior work has documented that certain capabilities appear abruptly once models cross specific scale thresholds <span id="id9">[<a class="reference internal" href="zbib.html#id288" title="Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? Advances in neural information processing systems, 36:55565–55581, 2023.">SMK23</a>, <a class="reference internal" href="zbib.html#id287" title="Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, and others. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.">WTB+22</a>]</span>. This challenges a simple reductionist explanations of model behavior. Explaining why these behaviors arise and how they work remain open research problems.</p>
<p><strong>The illusion of transparency</strong>. GenAI systems cannot be relied upon to explain their own decisions or reasoning processes. Although LLMs can generate fluent and plausible explanations when prompted, these explanations do not necessarily reflect the true internal mechanisms that produced the output. Empirical studies have shown that model-generated rationales may be post-hoc constructions rather than faithful accounts of the underlying computation <span id="id10">[<a class="reference internal" href="zbib.html#id289" title="Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: how should we define and evaluate faithfulness? arXiv preprint arXiv:2004.03685, 2020.">JG20</a>, <a class="reference internal" href="zbib.html#id290" title="Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36:74952–74965, 2023.">TMPB23</a>]</span>. Consequently, user-facing explanations produced by GenAI systems may sound “plausible” but can mislead users into overestimating model understanding or reliability.</p>
<p>Despite these challenges, there are substantial and growing research efforts aimed at improving the interpretability and transparency of GenAI systems. One direction involves extending existing interpretability techniques to GenAI systems. Some research has adapted feature attribution and probing techniques to sequential and generative settings to obtain partial insights into how language models process inputs and generate outputs <span id="id11">[<a class="reference internal" href="zbib.html#id291" title="Yonatan Belinkov. Probing classifiers: promises, shortcomings, and advances. Computational Linguistics, 48(1):207–219, 2022.">Bel22</a>, <a class="reference internal" href="zbib.html#id260" title="Joseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language models. arXiv preprint arXiv:2305.15853, 2023.">Eng23</a>]</span>.</p>
<p>A second, increasingly influential line of research focuses on <em>mechanistic interpretability</em>. Mechanistic interpretability aims to reverse-engineer neural networks by identifying interpretable computational circuits (such as attention heads, neurons, or activations), and mapping them to specific algorithmic or linguistic functions. Rather than treating models as black boxes to be explained post hoc, this approach treats trained models as artifacts to be dissected and understood at the level of internal mechanisms. We recommend reader to <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J">A Comprehensive Mechanistic Interpretability Explainer &amp; Glossary</a> and toolkits such as <a class="reference external" href="https://github.com/TransformerLensOrg/TransformerLens">TransformerLens</a> for more resources. Although still in its early stages, mechanistic interpretability represents a promising pathway toward deeper Model transparency for GenAI systems.</p>
</section>
<section id="genai-privacy-and-security">
<h3>GenAI, Privacy, and Security<a class="headerlink" href="#genai-privacy-and-security" title="Link to this heading">#</a></h3>
<p>GenAI systems introduce seismic challenges to security and privacy protection that go beyond those associated with standard machine learning models. These challenges create new attack surfaces and governance concerns.</p>
<p>The lack of transparency surrounding pre-training data also leads to serious privacy concerns. Even though the composition of these datasets is rarely disclosed in detail, there’s mounting evidence that personal data and copyrighted materials have been included without consent, and that LLMs can memorize and regurgitate sensitive information under certain conditions</p>
<p>These issues have moved beyond academic debate into the legal arena. Notably, The New York Times filed a high-profile lawsuit against OpenAI and Microsoft, alleging that its copyrighted articles were used in model training without authorization and that the resulting models can reproduce substantial portions of protected content <span id="id12">[<a class="reference internal" href="zbib.html#id292" title="Michael M Grynbaum and Ryan Mac. The times sues openai and microsoft over ai use of copyrighted work. The New York Times, 2023.">GM23</a>]</span>.</p>
<p>In addition to data-related risks, jailbreaking attacks represent a prominent and evolving security threat for GenAI systems. Jailbreaking refers to techniques that bypass built-in safeguards, content filters, or alignment constraints, enabling models to generate outputs that violate safety, security, or ethical guidelines. Recent research has shown that such attacks can be achieved through unexpected and creative prompting strategies. As two examples, asking LLMs to repeat the same word over and over again can cause it to leak personal data <span id="id13">[<a class="reference internal" href="zbib.html#id293" title="Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023.">NCH+23</a>]</span>, and the so-called “adversarial poetry” prompts have been shown to elicit restricted behaviors by embedding malicious intent within seemingly innocent poems <span id="id14">[<a class="reference internal" href="zbib.html#id271" title="Piercosma Bisconti, Matteo Prandi, Federico Pierucci, Francesco Giarrusso, Marcantonio Bracale, Marcello Galisai, Vincenzo Suriani, Olga Sorokoletova, Federico Sartore, and Daniele Nardi. Adversarial poetry as a universal single-turn jailbreak mechanism in large language models. arXiv preprint arXiv:2511.15304, 2025.">BPP+25</a>]</span>.</p>
<p>Recognizing the growing importance of these risks, institutions and organizations are actively developing guidelines and frameworks to strengthen security and privacy protections in the age of GenAI. In the public sector, the U.S. National Institute of Standards and Technology (NIST) has released new draft guidelines calling for a <a class="reference external" href="https://www.nist.gov/news-events/news/2025/12/draft-nist-guidelines-rethink-cybersecurity-ai-era">rethinking of cybersecurity practices in light of AI-driven systems</a>. In addition, industry actors such as Cisco has assembled a <a class="reference external" href="https://learn-cloudsecurity.cisco.com/ai-security-framework">AI security framework</a> that provides a comprehensive taxonomy of GenAI security and safety threats across the entire AI life-cycle.</p>
</section>
</section>
<section id="new-challenge-ai-alignment">
<h2>New Challenge: AI Alignment<a class="headerlink" href="#new-challenge-ai-alignment" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>If any company or group, anywhere on the planet, builds an artificial superintelligence using anything remotely like current techniques, based on anything remotely like the present understanding of AI, then everyone, everywhere on Earth, will die.  – Eliezer Yudkowsky, <a class="reference external" href="https://ifanyonebuildsit.com/">If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All</a></p>
</div></blockquote>
<p>AI alignment refers to the challenge of ensuring that advanced AI systems pursue goals, behaviors, and decision-making processes that are consistent with human values, intentions, and societal norms <span id="id15">[<a class="reference internal" href="zbib.html#id294" title="Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2014.">Bos14</a>]</span>. As AI systems grow more capable, small mismatches between intended and learned objectives can be amplified in unexpected ways, leading to harmful or undesired outcomes even when the system appears competent or well-behaved. In GenAI systems, current efforts center on using reinforcement learning techniques to align GenAI with human preferences.</p>
<section id="a-very-brief-introduction-to-reinforcement-learning">
<h3>A Very Brief Introduction to Reinforcement Learning<a class="headerlink" href="#a-very-brief-introduction-to-reinforcement-learning" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>Reinforcement learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal.  – Sutton and Barto, <em>Reinforcement Learning</em> (Chapter 1.1)</p>
</div></blockquote>
<p><strong>Reinforcement learning</strong> is the paradigm of learning from interactions, about how to act under what situations. In a sense, RL is closer to how humans learn various skills: we learn by exploring and interacting with the world around us.</p>
<p>Reinforcement learning has been successfully applied in game playing, robotics, and aligning large language models.</p>
<p>A reinforcement learning system is often described by the following agent-environment framework.</p>
<p><img alt="The Agent-Environment Framework" src="../_images/agent_environment.jpg" /></p>
<p>Let’s look at each component:</p>
<ul class="simple">
<li><p><strong>Agent</strong> is the decision-maker who takes certain <strong>action</strong> in a given situation;</p></li>
<li><p><strong>Environment</strong> contains everything outside the agent. It is what the agent interacts with.</p></li>
<li><p>In response to the agent’s action, the environment produces a <strong>reward</strong>. The agent tries to learn from the reward feedback to figure out what’s the best action(s);</p></li>
<li><p>The environment is characterized by its <strong>states</strong>, which, roughly speaking, are the “situations” that the agent is facing. The states may change as a result of the agent’s actions, and the states also affect the agent’s actions;</p></li>
<li><p>A mapping between the agent’s actions and the environment’s states is called the <strong>policy</strong>. RL algorithms are trying to learn <strong>optimal policy</strong> from interactions.</p></li>
</ul>
<p>In the context of GenAI systems, the agent is the GenAI model and the environment contains users who interact with the model. The state(s) correspond to a user prompt (as well as other context information) and the agent’s action involves generating a response, and the mapping from states (prompt and context) to actions (response) is the policy. The rewards are signals (often provided by humans) that encode preference for certain responses over others.</p>
</section>
<section id="why-is-alignment-hard">
<h3>Why Is Alignment Hard?<a class="headerlink" href="#why-is-alignment-hard" title="Link to this heading">#</a></h3>
<p>All of the aforementioned GenAI-induced challenges, including opacity of training data and model mechanisms, inherent ambiguity of human preferences, potential trade-off between model capability and alignment with certain objectives, clearly contribute to the difficulty of AI alignment. In this sense, alignment is best understood as a socio-technical challenge that spans across problem formulation, data curation, model training, and decision governance. Here, we would like to highlight two other relevant perspectives.</p>
<p>First, <strong>reward hacking</strong> reduces the effectiveness of reinforcement learning for AI alignment. Reward hacking happens when an AI system learns to maximize its reward signal in unintended or undesirable ways, e.g., by discovering strategies that exploit loopholes how reward signals can be maximized while violating the underlying human intent. For GenAI systems, reward hacking can manifest as generating superficially compliant or persuasive outputs that score high in alignment evaluations without genuinely satisfying the spirit of alignment goals <span id="id16">[<a class="reference internal" href="zbib.html#id295" title="Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.">AOS+16</a>]</span>. More deeply, this reflects the inherent difficulty of precisely specifying human preferences via simple reward signals.</p>
<p>Second, <strong>emergent misalignment</strong> is an interesting phenomenon where alignment failures arise as unintended side effects of seemingly narrow or benign interventions. Recent work has shown that fine-tuning LLMs on limited objectives can induce broad and unexpected misalignment across behaviors that were not directly optimized. For example, <span id="id17">Betley <em>et al.</em> [<a class="reference internal" href="zbib.html#id296" title="Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, and Owain Evans. Emergent misalignment: narrow finetuning can produce broadly misaligned llms. arXiv preprint arXiv:2502.17424, 2025.">BTW+25</a>]</span> demonstrates that fine-tuning a model on generating insecure code can lead the model to exhibit misaligned behavior across a wide range of prompts unrelated to coding (e.g., engage in deception). Emergent misalignment highlights the limitation of local alignment fixes, which may affect the model in unexpected ways.</p>
</section>
</section>
<section id="new-challenge-robustness">
<h2>New Challenge: Robustness<a class="headerlink" href="#new-challenge-robustness" title="Link to this heading">#</a></h2>
<p>There is growing evidence suggesting that GenAI systems lack a basic level of robustness. Even when models appear highly capable, their behaviors may be unstable under input perturbations. Recent work has documented that LLMs can exhibit surprising sensitivity to superficial or semantically irrelevant features of a task. For example, in mathematical reasoning, <span id="id18">Mirzadeh <em>et al.</em> [<a class="reference internal" href="zbib.html#id298" title="Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024.">MAS+24</a>]</span> shows that LLMs’ answers can change simply by altering inconsequential details such as the name of a person in a word problem, despite the underlying mathematical structure remaining identical. Such behavior raises concerns about whether models are truly reasoning over abstract problem structure or instead relying on brittle pattern matching. Similar issues arise in strategic and behavioral modeling contexts. Research by <span id="id19">Gao <em>et al.</em> [<a class="reference internal" href="zbib.html#id297" title="Yuan Gao, Dokyun Lee, Gordon Burtch, and Sina Fazelpour. Take caution in using llms as human surrogates. Proceedings of the National Academy of Sciences, 122(24):e2501660122, 2025.">GLBF25</a>]</span> cautions against treating LLMs as faithful surrogates for human decision-makers, showing that model behavior can shift depending on the language (English vs. Chinese) in which the task is described. Finally, <span id="id20">Gonen <em>et al.</em> [<a class="reference internal" href="zbib.html#id272" title="Hila Gonen, Terra Blevins, Alisa Liu, Luke Zettlemoyer, and Noah A Smith. Does liking yellow imply driving a school bus? semantic leakage in language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 785–798. 2025.">GBL+25</a>]</span> demonstrates that models may inadvertently encode and exploit correlations between labels and input semantics that leak information unrelated to the task objective (e.g., associating the color “yellow” with someone’s occupation of being a “school bus driver”). This lack of robustness undermines trust in GenAI systems when applied in high-stakes applications.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="3-2-mechanism.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Privacy Mechanisms</p>
      </div>
    </a>
    <a class="right-next"
       href="resources.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Resources</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#same-problems-new-challenges">Same Problems, New Challenges</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#genai-and-fairness">GenAI and Fairness</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#genai-and-interpretability">GenAI and Interpretability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#genai-privacy-and-security">GenAI, Privacy, and Security</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-challenge-ai-alignment">New Challenge: AI Alignment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-very-brief-introduction-to-reinforcement-learning">A Very Brief Introduction to Reinforcement Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-alignment-hard">Why Is Alignment Hard?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-challenge-robustness">New Challenge: Robustness</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gediminas Adomavicius and Mochen Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>