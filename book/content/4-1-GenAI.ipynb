{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI in the Age of Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative AI (GenAI) systems, such as large language models, represent arguably one of the most exciting recent advances in the field of AI. The same responsible AI issues, including fairness, interpretability, and privacy, remain highly relevant to GenAI, and they sometimes take on different forms. Meanwhile, GenAI also presents unique challenges, related to the broader alignment issue.\n",
    "\n",
    "This chapter provides a brief exposition of responsible AI in GenAI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Problems, New Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI and Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GenAI systems have been shown to exhibit biased generations, likely due to existing bias during pre-training;\n",
    "- This bias is hard to remove / rectify after the fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI and Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> People outside the field are often surprised and alarmed to learn that we do not understand how our own AI creations work. -- Dario Amodei, [The Urgency of Interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenAI systems are exceedingly hard to make sense of:\n",
    "- Anthropic quote\n",
    "- mechanistic interpretation\n",
    "- illusion of interpretability as responses from GenAI systems themselves\n",
    "\n",
    "explaining language models {cite:t}`enguehard2023sequential`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI and Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenAI brings new challenges to privacy protection:\n",
    "- lack of transparency to data used during pre-training;\n",
    "- prompt hacking / injection reveals personal information\n",
    "- tradeoff between automation and privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jailbreaking represents a prominent threat for GenAI model. Jailbreaking can sometimes be achieved via surprising routes (e.g., \"adversarial poetry\", {cite:p}`bisconti2025adversarial`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A New Problem: AI Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yodovsky quote\n",
    "- Definition of AI Aligment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Very Brief Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\"Reinforcement learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal.\"_ - \\[Sutton and Barto, Chapter 1.1\\] \n",
    "\n",
    "**Reinforcement learning** is the paradigm of learning from interactions, about how to act under what situations. In a sense, RL is closer to how humans learn various skills: we learn by exploring and interacting with the world around us.\n",
    "\n",
    "Reinforcement learning has been successfully applied in game playing, robotics, and aligning large language models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reinforcement learning system is often described by the following agent-environment framework.\n",
    "\n",
    "![The Agent-Environment Framework](../images/agent_environment.jpg)\n",
    "\n",
    "Let's look at each component:\n",
    "- **Agent** is the decision-maker who takes certain **action** in a given situation;\n",
    "- **Environment** contains everything outside the agent. It is what the agent interacts with.\n",
    "- In response to the agent's action, the environment produces a **reward**. The agent tries to learn from the reward feedback to figure out what's the best action(s);\n",
    "- The environment is characterized by its **states**, which, roughly speaking, are the \"situations\" that the agent is facing. The states may change as a result of the agent's actions, and the states also affect the agent's actions;\n",
    "- Each action that the agent can take has certain **value**, which describes how good it is in achieving high reward;\n",
    "- A mapping between the agent's actions and the environment's states is called the **policy**. RL algorithms are trying to learn **optimal policy** from interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental issue in reinforcement learning is the _tradeoff between exploration vs. exploitation_. Roughly speaking, **Exploration** means trying out different possible actions to see which one(s) work better; and **Exploitation** means keeping taking the action that appears to be the best based on currently available information. \n",
    "\n",
    "Balancing exploration and exploitation is the central consideration of most reinforcement learning algorithm design. Having too much exploration is wasteful and having too much exploitation can be myopic (maybe there is a much better option that you haven't tried).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Alignment Hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward hacking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
