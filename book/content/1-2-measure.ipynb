{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to addressing algorithmic bias is to be able to measure the degree of fairness (or lack thereof) given a model's predictions on a certain dataset. This chapter covers a number of commonly used fairness measures. For concreteness, we will demonstrate these fairness measures by calculating them on the [Compas Recidivism Dataset](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background of COMPAS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recidivism prediction refers to the task of estimating the likelihood that a convicted criminal will reoffend after being released. This type of prediction plays a critical role in the criminal justice system, influencing decisions about bail, parole, sentencing, and rehabilitation programs. In recent years, machine learning (ML) models have been increasingly deployed to assist with these predictions, promising greater objectivity and efficiency. One prominent example is the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool, developed by Northpointe Inc. COMPAS uses a range of input features, including criminal history, age, and responses to interview questions, to generate risk scores intended to inform judicial decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the deployment of COMPAS and similar tools has sparked considerable controversy. In 2016, ProPublica published an investigative article titled \"[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing),\" which analyzed COMPAS scores and concluded that the tool was racially biased. According to their analysis, Black defendants were disproportionately labeled as high risk compared to white defendants, even when they did not reoffend. Conversely, white defendants were more often mislabeled as low risk. ProPublica argued that this disparity indicated systemic racial bias embedded in the algorithm (or more technically, that the algorithm violates a specific notion of fairness known as **error balance**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Northpointe Inc. responded to these claims by challenging ProPublica's methodology and interpretation. They argued that COMPAS was calibrated to achieve equal predictive accuracy across racial groups and emphasized that fairness should be understood in terms of **predictive parity**—meaning that individuals with the same score should have the same likelihood of reoffending, regardless of race. This rebuttal highlighted the complex and often conflicting definitions of fairness in algorithmic decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error balance and predictive parity (along with demographic parity) are among the most commonly used fairness metrics. Next, we will demonstrate these different fairness metrics on (a subset of) the COMPAS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Data: Compas Recidivism Dataset\n",
    ":class: note\n",
    "- Location: \"data/compas-scores-two-years.csv\"\n",
    "- Shape: (7214, 53)\n",
    "- Note: data taken from ProPublica's analyses of COMPAS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script imports the dataset and prints the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>...</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>marcu brown</td>\n",
       "      <td>marcu</td>\n",
       "      <td>brown</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-01-21</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>bouthy pierrelouis</td>\n",
       "      <td>bouthy</td>\n",
       "      <td>pierrelouis</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Male</td>\n",
       "      <td>1973-01-22</td>\n",
       "      <td>43</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                name   first         last compas_screening_date   sex  \\\n",
       "0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male   \n",
       "1   3         kevon dixon   kevon        dixon            2013-01-27  Male   \n",
       "2   4            ed philo      ed        philo            2013-04-14  Male   \n",
       "3   5         marcu brown   marcu        brown            2013-01-13  Male   \n",
       "4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male   \n",
       "\n",
       "          dob  age          age_cat              race  ...  v_decile_score  \\\n",
       "0  1947-04-18   69  Greater than 45             Other  ...               1   \n",
       "1  1982-01-22   34          25 - 45  African-American  ...               1   \n",
       "2  1991-05-14   24     Less than 25  African-American  ...               3   \n",
       "3  1993-01-21   23     Less than 25  African-American  ...               6   \n",
       "4  1973-01-22   43          25 - 45             Other  ...               1   \n",
       "\n",
       "   v_score_text  v_screening_date  in_custody  out_custody  priors_count.1  \\\n",
       "0           Low        2013-08-14  2014-07-07   2014-07-14               0   \n",
       "1           Low        2013-01-27  2013-01-26   2013-02-05               0   \n",
       "2           Low        2013-04-14  2013-06-16   2013-06-16               4   \n",
       "3        Medium        2013-01-13         NaN          NaN               1   \n",
       "4           Low        2013-03-26         NaN          NaN               2   \n",
       "\n",
       "  start   end event two_year_recid  \n",
       "0     0   327     0              0  \n",
       "1     9   159     1              1  \n",
       "2     0    63     0              1  \n",
       "3     0  1174     0              0  \n",
       "4     0  1102     0              0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import compas two-year dataset\n",
    "import pandas as pd\n",
    "compas = pd.read_csv('../data/compas-scores-two-years.csv')\n",
    "compas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6172, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# following the ProPublica analysis, we remove several rows with missing data\n",
    "# see https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb for more details\n",
    "compas = compas[['age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count', 'days_b_screening_arrest', 'decile_score', 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out']]\n",
    "compas = compas[(compas['days_b_screening_arrest'] <= 30) & \n",
    "                (compas['days_b_screening_arrest'] >= -30) &  \n",
    "                (compas['is_recid'] != -1) &\n",
    "                (compas['c_charge_degree'] != 'O') & \n",
    "                (compas['score_text'] != 'N/A')]\n",
    "compas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5278, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we focus on \"race\" as the protected attribute and African American vs. Caucasian as the two groups\n",
    "compas = compas[compas['race'].isin(['African-American', 'Caucasian'])]\n",
    "compas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Measures for Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we consider measuring fairness of a classification model. In the ```Compas``` dataset, the ```score_text``` columns contains model-predicted risk level (low, medium, high) and the ```two_year_recid``` column contains the actual two-year recidivism label (1, 0). Because the ground-truth outcome label is binary, we focus on the \"low\" and \"high\" predicted classes here.[^footnote1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compas_binary = compas.copy()\n",
    "# remove \"medium\" in score_text\n",
    "compas_binary = compas_binary[compas_binary['score_text'] != 'Medium']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this chapter, we adopt the following notations in formal definitions of different fairness measures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Notations\n",
    ":class: tip\n",
    "- $Y$: actual outcome (0/1, recidivism within two years or not);\n",
    "- $\\widehat{Y}$: predicted risk score (\"High\" or \"Low\");\n",
    "- $p$: predicted probability of being \"High\" risk;\n",
    "- $R$: race, protected attribute of interested in this example. $R \\in \\{AA, W\\}$ represents African American and Caucasian respectively;\n",
    "- $\\boldsymbol{X}$: other observable characteristics (e.g., gender, age, prior offenses).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demographic parity** (sometimes also referred to as **Statistical Parity**) is one of the most straightforward fairness metrics. It simply asserts that one group should not receive systematically more favorable predicted outcomes than the other group. Despite its simplicity, it has been extensively used / discussed in the prior literature, such as {cite:t}`calders2010three,calders2009building,kamiran2009classifying,kamishima2011fairness`, among many others.\n",
    "\n",
    "In the context of recidivism prediction, with the protected attribute being race and the two groups to be compared being African American vs. White, demographic parity can be defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Demographic Parity for Binary Classifier\n",
    ":class: tip\n",
    "$$\n",
    "\\Pr(\\widehat{Y} = High | R = AA) = \\Pr(\\widehat{Y} = High | R = W)\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 'high' predicted scores among African Americans:  0.38566864445458693\n",
      "Percentage of 'high' predicted scores among Whites:  0.13680981595092023\n"
     ]
    }
   ],
   "source": [
    "# evaluate demographic parity\n",
    "# percentage of \"high\" scores among African Americans\n",
    "AA_pct = len(compas_binary[(compas_binary['score_text'] == 'High') & (compas_binary['race'] == 'African-American')]) / len(compas_binary[compas_binary['race'] == 'African-American'])\n",
    "# percentage of \"high\" scores among Whites\n",
    "W_pct = len(compas_binary[(compas_binary['score_text'] == 'High') & (compas_binary['race'] == 'Caucasian')]) / len(compas_binary[compas_binary['race'] == 'Caucasian'])\n",
    "print(\"Percentage of 'high' predicted scores among African Americans: \", AA_pct)\n",
    "print(\"Percentage of 'high' predicted scores among Whites: \", W_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, around 38.6\\% of African American defendents received high-risk predictions whereas only 13.7\\% of White defendents received high-risk predictions. This is a violation of demographic parity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Demographic Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demographic parity is a rather crude measure and, specifically, does not take into account any systematic differences across the two groups that may legitimately explain some of the disparity in predicted outcomes. **Conditional Demographic Parity** seeks to amend this issue by conditioning on other observable characteristics. It has also been discussed in prior literature, including for example, {cite:t}`vzliobaite2011handling,kamiran2013quantifying`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Conditional Demographic Parity for Binary Classifier\n",
    ":class: tip\n",
    "$$\n",
    "\\Pr(\\widehat{Y} = High | R = AA, \\boldsymbol{X}) = \\Pr(\\widehat{Y} = High | R = W, \\boldsymbol{X})\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate conditional demographic parity\n",
    "# following ProPublica's analysis, X may contain: age, gender, number of prior offenses, and severity of charge\n",
    "# they can be used as control variables in a logistic regression of predicted risk score on race\n",
    "import statsmodels.formula.api as smf\n",
    "# convert score_text to 1 and 0\n",
    "compas_binary['Y'] = compas_binary['score_text'].apply(lambda x: 1 if x == 'High' else 0)\n",
    "model = smf.logit(formula = \"Y ~ race + age + sex + priors_count + c_charge_degree\", data = compas_binary).fit(disp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td colspan=\"1\"><em>Dependent variable: Predicted High Risk</em></td></tr><tr><td style=\"text-align:left\"></td>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "\n",
       "<tr><td style=\"text-align:left\">Intercept</td><td>2.582<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.224)</td></tr>\n",
       "<tr><td style=\"text-align:left\">age</td><td>-0.140<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.007)</td></tr>\n",
       "<tr><td style=\"text-align:left\">Charge Degree Misdemeanor</td><td>-0.495<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.106)</td></tr>\n",
       "<tr><td style=\"text-align:left\">priors_count</td><td>0.378<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.016)</td></tr>\n",
       "<tr><td style=\"text-align:left\">race Caucasian</td><td>-0.653<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.105)</td></tr>\n",
       "<tr><td style=\"text-align:left\">sex Male</td><td>-0.037<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.128)</td></tr>\n",
       "\n",
       "<td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align: left\">Observations</td><td>3821</td></tr><tr><td style=\"text-align: left\">Pseudo R<sup>2</sup></td><td>0.392</td></tr>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td><td colspan=\"1\" style=\"text-align: right\"><sup>*</sup>p&lt;0.1; <sup>**</sup>p&lt;0.05; <sup>***</sup>p&lt;0.01</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stargazer.stargazer import Stargazer\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "sg = Stargazer([model])\n",
    "sg.dependent_variable_name('Predicted High Risk')\n",
    "sg.rename_covariates({\n",
    "    \"c_charge_degree[T.M]\": \"Charge Degree Misdemeanor\",\n",
    "    \"race[T.Caucasian]\": \"race Caucasian\",\n",
    "    \"sex[T.Male]\": \"sex Male\"\n",
    "})\n",
    "sg.show_model_numbers(False)\n",
    "display(HTML(sg.render_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even after controlling for age, gender, prior offenses and the severity of charge, being African American still significantly increased the odds of receiving high-risk predictions. This is a violation of conditional demographic parity.\n",
    "\n",
    "Demographic parity (with or without conditioning on covariates) concerns only with a model's predictions but not the ground truths. Error balance and predictive parity are two representative fairness metrics that account for both, which are at the center of the debate between ProPublica and Northpointe. We discuss them next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Balance / Equalized Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demographic parity (with or without conditioning on other observable characteristics) aims to balance the predicted outcomes of two groups. Fairness measures like these reflect the _Equal Outcome_ ideal. In contrast, many other fairness measures are designed to reflect the _Equal Opportunity_ ideal (that individuals from different groups who are equally \"eligible\" are treated equally). **Error Balance** or **Equalized Odds** is one of such measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error balance requires that the false positive rate and false negative rate are equalized for the two groups. Because false positive / negative rates are equivalent to the complements of recall rates for positive / negative classes, the error balance metric is the same as **equalizing recall rates** for positive and negative classes across the two groups. More formally,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Error Balance / Equalized Odds\n",
    ":class: tip\n",
    "\\begin{align*}\n",
    "\\Pr(\\widehat{Y} = High | R = AA, Y = 0) & = \\Pr(\\widehat{Y} = High | R = W, Y = 0) \\\\\n",
    "\\Pr(\\widehat{Y} = Low | R = AA, Y = 1) & = \\Pr(\\widehat{Y} = Low | R = W, Y = 1)\n",
    "\\end{align*}\n",
    "\n",
    "or equivalently\n",
    "\\begin{align*}\n",
    "\\text{Recall(High | R = AA)} & = \\text{Recall(High | R = W)} \\\\\n",
    "\\text{Recall(Low | R = AA)} & = \\text{Recall(Low | R = W)}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate for African Americans:  0.19464944649446494\n",
      "False Positive Rate for Whites:  0.057547169811320756\n",
      "False Negative Rate for African Americans:  0.42728093947606144\n",
      "False Negative Rate for Whites:  0.7157894736842105\n"
     ]
    }
   ],
   "source": [
    "# evaluate FPR and FNR for both groups\n",
    "FPR_AA = len(compas_binary[(compas_binary['score_text'] == 'High') & (compas_binary['race'] == 'African-American') & (compas_binary['two_year_recid'] == 0)]) / len(compas_binary[(compas_binary['race'] == 'African-American') & (compas_binary['two_year_recid'] == 0)])\n",
    "FNR_AA = len(compas_binary[(compas_binary['score_text'] == 'Low') & (compas_binary['race'] == 'African-American') & (compas_binary['two_year_recid'] == 1)]) / len(compas_binary[(compas_binary['race'] == 'African-American') & (compas_binary['two_year_recid'] == 1)])\n",
    "FPR_W = len(compas_binary[(compas_binary['score_text'] == 'High') & (compas_binary['race'] == 'Caucasian') & (compas_binary['two_year_recid'] == 0)]) / len(compas_binary[(compas_binary['race'] == 'Caucasian') & (compas_binary['two_year_recid'] == 0)])\n",
    "FNR_W = len(compas_binary[(compas_binary['score_text'] == 'Low') & (compas_binary['race'] == 'Caucasian') & (compas_binary['two_year_recid'] == 1)]) / len(compas_binary[(compas_binary['race'] == 'Caucasian') & (compas_binary['two_year_recid'] == 1)])\n",
    "# print results\n",
    "print(\"False Positive Rate for African Americans: \", FPR_AA)\n",
    "print(\"False Positive Rate for Whites: \", FPR_W)\n",
    "print(\"False Negative Rate for African Americans: \", FNR_AA)\n",
    "print(\"False Negative Rate for Whites: \", FNR_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the false positive / negative rate is clearly higher / lower for African Americans. Specifically, the model makes more false positive mistakes (i.e., misclassify a low-risk defendent as high-risk) for African Americans but makes more false negative mistakes (i.e., misclassify a high-risk defendent as low-risk) for Whites. In fact, this is one of ProPublica's main arguments to show the unfairness of COMPAS {cite:p}`angwin2016machine`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the case of demographic parity, you can further condition on other observables to construct a **conditional** version of error balance / equalized odds. One way to test whether error balance is violated is to create subsets of data where the ground truth recidivism label is 0 or 1. Within each subsample, run logistic regressions of the model's predictions on race and other observed characteristics, then check the coefficient estimate on the group identifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two subsamples, respectively for two-year recidivism label 0 and 1\n",
    "compas_recid_0 = compas_binary[compas_binary['two_year_recid'] == 0]\n",
    "compas_recid_1 = compas_binary[compas_binary['two_year_recid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressions on recall of class 0\n",
    "model_recid_0 = smf.logit(formula = \"Y ~ race + age + sex + priors_count + c_charge_degree\", data = compas_recid_0).fit(disp=0)\n",
    "# regressions on recall of class 1\n",
    "model_recid_1 = smf.logit(formula = \"Y ~ race + age + sex + priors_count + c_charge_degree\", data = compas_recid_1).fit(disp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable: Predicted High Risk</em></td></tr><tr><td style=\"text-align:left\"></td><tr><td></td><td colspan=\"1\">Recidivism = 0</td><td colspan=\"1\">Recidivism = 1</td></tr>\n",
       "<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "\n",
       "<tr><td style=\"text-align:left\">Intercept</td><td>1.737<sup>***</sup></td><td>2.948<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.350)</td><td>(0.300)</td></tr>\n",
       "<tr><td style=\"text-align:left\">age</td><td>-0.127<sup>***</sup></td><td>-0.129<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.011)</td><td>(0.009)</td></tr>\n",
       "<tr><td style=\"text-align:left\">Charge Degree Misdemeanor</td><td>-0.209<sup></sup></td><td>-0.681<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.166)</td><td>(0.140)</td></tr>\n",
       "<tr><td style=\"text-align:left\">priors_count</td><td>0.373<sup>***</sup></td><td>0.328<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.028)</td><td>(0.020)</td></tr>\n",
       "<tr><td style=\"text-align:left\">race Caucasian</td><td>-0.819<sup>***</sup></td><td>-0.526<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.172)</td><td>(0.139)</td></tr>\n",
       "<tr><td style=\"text-align:left\">sex Male</td><td>-0.101<sup></sup></td><td>-0.134<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.190)</td><td>(0.184)</td></tr>\n",
       "\n",
       "<td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align: left\">Observations</td><td>2144</td><td>1677</td></tr><tr><td style=\"text-align: left\">Pseudo R<sup>2</sup></td><td>0.301</td><td>0.345</td></tr>\n",
       "<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td><td colspan=\"2\" style=\"text-align: right\"><sup>*</sup>p&lt;0.1; <sup>**</sup>p&lt;0.05; <sup>***</sup>p&lt;0.01</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stargazer.stargazer import Stargazer\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "sg = Stargazer([model_recid_0, model_recid_1])\n",
    "sg.dependent_variable_name('Predicted High Risk')\n",
    "sg.rename_covariates({\n",
    "    \"c_charge_degree[T.M]\": \"Charge Degree Misdemeanor\",\n",
    "    \"race[T.Caucasian]\": \"race Caucasian\",\n",
    "    \"sex[T.Male]\": \"sex Male\"\n",
    "})\n",
    "sg.show_model_numbers(False)\n",
    "sg.custom_columns(['Recidivism = 0', 'Recidivism = 1'], [1, 1])\n",
    "display(HTML(sg.render_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression on the ```compas_recid_0``` sample, we see a significant and negative coefficient on the race variable, indicating that white defendants who did not reoffend have a lower probability of experiencing false positive predictions (i.e., recall rate of class 0 is higher for white defendants). Similarly, in the regression on the ```compas_recid_1``` sample, we also see a significant and negative coefficient on the race variable, indicating that white defendants who did reoffend have a higher probability of experiencing false negative predictions (i.e., recall rate of class 1 is lower for white defendants)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Importantly, error balance is sometimes informally described as requiring a classifier to not be systematically **more inaccurate** for one group vs. the other group. This can be misleading. Error balance focus on equalizing a specific type of performance metric, namely the recall rates. As will be seen in the subsequent discussion on predictive parity, even when error balance is satisfied, the model can still have differential accuracy for different groups, e.g., by achieving different precision rates.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to define error balance makes use of **predicted probabilities** rather than class predictions. It seeks to equalize the expected predicted probabilities of different groups conditioning on true labels (see, for example, {cite:t}`kleinberg2016inherent`). More formally,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Error Balance / Equalized Odds (with predicted probabilities)\n",
    ":class: tip\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(p | R = AA, Y = 0) & = \\mathbb{E}(p | R = W, Y = 0) \\\\\n",
    "\\mathbb{E}(p | R = AA, Y = 1) & = \\mathbb{E}(p | R = W, Y = 1)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average decile score for African Americans who did not recidivate:  3.5488929889298895\n",
      "Average decile score for African Americans who recidivated:  6.308039747064138\n",
      "Average decile score for Whites who did not recidivate:  2.3650943396226416\n",
      "Average decile score for Whites who recidivated:  4.187719298245614\n"
     ]
    }
   ],
   "source": [
    "# evaluate error balance with predicted probabilities\n",
    "# we will use \"decile_score\" to approximate the predicted probability of recidivism\n",
    "# COMPAS predicts \"High\" risk when decile score is greater than 7 and \"Low\" if the decile score is lower than 5\n",
    "# print average decile_score for African Americans and Whites\n",
    "AA_0_avg = compas_binary[(compas_binary['race'] == 'African-American') & (compas_binary['two_year_recid'] == 0)]['decile_score'].mean()\n",
    "AA_1_avg = compas_binary[(compas_binary['race'] == 'African-American') & (compas_binary['two_year_recid'] == 1)]['decile_score'].mean()\n",
    "W_0_avg = compas_binary[(compas_binary['race'] == 'Caucasian') & (compas_binary['two_year_recid'] == 0)]['decile_score'].mean()\n",
    "W_1_avg = compas_binary[(compas_binary['race'] == 'Caucasian') & (compas_binary['two_year_recid'] == 1)]['decile_score'].mean()\n",
    "# print results\n",
    "print(\"Average decile score for African Americans who did not recidivate: \", AA_0_avg)\n",
    "print(\"Average decile score for African Americans who recidivated: \", AA_1_avg)\n",
    "print(\"Average decile score for Whites who did not recidivate: \", W_0_avg)\n",
    "print(\"Average decile score for Whites who recidivated: \", W_1_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that the model systematically assigned higher decile scores to African Americans who did not recidivate, but lower scores to Whites who did recidivate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to ProPublica's allegation that COMPAS violated error balance, {cite:t}`dieterich2016compas` argued that a more \"appropriate\" fairness metric would be **Predictive Parity**, which requires that individuals who received a given type of predictions (high risk or low risk) should have equal probability of actually reoffending or not, regardless of race. In machine learning language, predictive parity requires equalizing the **precision rates** of both classes across groups. Formally, it can be defined as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Predictive Parity (Equalized False Discovery / Omission Rates)\n",
    ":class: tip\n",
    "\\begin{align*}\n",
    "\\Pr(Y = 1 | R = AA, \\widehat{Y} = Low) & = \\Pr(Y = High | R = W, \\widehat{Y} = Low) \\\\\n",
    "\\Pr(Y = 0 | R = AA, \\widehat{Y} = High) & = \\Pr(Y = Low | R = W, \\widehat{Y} = High)\n",
    "\\end{align*}\n",
    "\n",
    "or equivalently\n",
    "\\begin{align*}\n",
    "\\text{Precision(High | R = AA)} & = \\text{Precision(High | R = W)} \\\\\n",
    "\\text{Precision(Low | R = AA)} & = \\text{Precision(Low | R = W)}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Omission Rate for African Americans:  0.3514115898959881\n",
      "False Omission Rate for Whites:  0.2899786780383795\n",
      "False Discovery Rate for African Americans:  0.24970414201183433\n",
      "False Discovery Rate for Whites:  0.273542600896861\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive parity for both groups\n",
    "FOR_AA = len(compas_binary[(compas_binary['two_year_recid'] == 1) & (compas_binary['race'] == 'African-American') & (compas_binary['score_text'] == 'Low')]) / len(compas_binary[(compas_binary['race'] == 'African-American') & (compas_binary['score_text'] == 'Low')])\n",
    "FDR_AA = len(compas_binary[(compas_binary['two_year_recid'] == 0) & (compas_binary['race'] == 'African-American') & (compas_binary['score_text'] == 'High')]) / len(compas_binary[(compas_binary['race'] == 'African-American') & (compas_binary['score_text'] == 'High')])\n",
    "FOR_W = len(compas_binary[(compas_binary['two_year_recid'] == 1) & (compas_binary['race'] == 'Caucasian') & (compas_binary['score_text'] == 'Low')]) / len(compas_binary[(compas_binary['race'] == 'Caucasian') & (compas_binary['score_text'] == 'Low')])\n",
    "FDR_W = len(compas_binary[(compas_binary['two_year_recid'] == 0) & (compas_binary['race'] == 'Caucasian') & (compas_binary['score_text'] == 'High')]) / len(compas_binary[(compas_binary['race'] == 'Caucasian') & (compas_binary['score_text'] == 'High')])\n",
    "# print results\n",
    "print(\"False Omission Rate for African Americans: \", FOR_AA)\n",
    "print(\"False Omission Rate for Whites: \", FOR_W)\n",
    "print(\"False Discovery Rate for African Americans: \", FDR_AA)\n",
    "print(\"False Discovery Rate for Whites: \", FDR_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we find that the false omission rate (i.e., labeled as low-risk, but did recidivate) is somewhat higher for African American than for White, and the false discovery rate is fairly similar. To account for other observed characteristics, we can create subsets of data where the predicted label is 0 or 1. Within each subsample, run logistic regressions of the actual recidivism outcome on race and other observed characteristics, then check the coefficient estimate on the group identifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two subsamples, respectively for predicted label 0 and 1\n",
    "compas_pred_0 = compas_binary[compas_binary['score_text'] == 'Low']\n",
    "compas_pred_1 = compas_binary[compas_binary['score_text'] == 'High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressions on precision of class 0\n",
    "model_pred_0 = smf.logit(formula = \"two_year_recid ~ race + age + sex + priors_count + c_charge_degree\", data = compas_pred_0).fit(disp=0)\n",
    "# regressions on precision of class 1\n",
    "model_pred_1 = smf.logit(formula = \"two_year_recid ~ race + age + sex + priors_count + c_charge_degree\", data = compas_pred_1).fit(disp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable: Recidivism</em></td></tr><tr><td style=\"text-align:left\"></td><tr><td></td><td colspan=\"1\">Predicted Low Risk</td><td colspan=\"1\">Predicted High Risk</td></tr>\n",
       "<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "\n",
       "<tr><td style=\"text-align:left\">Intercept</td><td>-0.145<sup></sup></td><td>1.464<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.169)</td><td>(0.314)</td></tr>\n",
       "<tr><td style=\"text-align:left\">age</td><td>-0.033<sup>***</sup></td><td>-0.047<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.004)</td><td>(0.009)</td></tr>\n",
       "<tr><td style=\"text-align:left\">Charge Degree Misdemeanor</td><td>0.056<sup></sup></td><td>-0.373<sup>**</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.088)</td><td>(0.167)</td></tr>\n",
       "<tr><td style=\"text-align:left\">priors_count</td><td>0.186<sup>***</sup></td><td>0.108<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.018)</td><td>(0.015)</td></tr>\n",
       "<tr><td style=\"text-align:left\">race Caucasian</td><td>-0.045<sup></sup></td><td>0.133<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.087)</td><td>(0.180)</td></tr>\n",
       "<tr><td style=\"text-align:left\">sex Male</td><td>0.353<sup>***</sup></td><td>0.452<sup>**</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.111)</td><td>(0.197)</td></tr>\n",
       "\n",
       "<td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align: left\">Observations</td><td>2753</td><td>1068</td></tr><tr><td style=\"text-align: left\">Pseudo R<sup>2</sup></td><td>0.059</td><td>0.065</td></tr>\n",
       "<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td><td colspan=\"2\" style=\"text-align: right\"><sup>*</sup>p&lt;0.1; <sup>**</sup>p&lt;0.05; <sup>***</sup>p&lt;0.01</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stargazer.stargazer import Stargazer\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "sg = Stargazer([model_pred_0, model_pred_1])\n",
    "sg.dependent_variable_name('Recidivism')\n",
    "sg.rename_covariates({\n",
    "    \"c_charge_degree[T.M]\": \"Charge Degree Misdemeanor\",\n",
    "    \"race[T.Caucasian]\": \"race Caucasian\",\n",
    "    \"sex[T.Male]\": \"sex Male\"\n",
    "})\n",
    "sg.show_model_numbers(False)\n",
    "sg.custom_columns(['Predicted Low Risk', 'Predicted High Risk'], [1, 1])\n",
    "display(HTML(sg.render_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, in both regressions, the coefficient estimate on race is non-siginificant, supporting Northpointe's argument that COMPAS achieved predictive parity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Impossibility of Achieving both Error Balance and Predictive Parity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COMPAS case is particularly complicated because, as it turns out, one cannot simultaneously achieve error balance and predictive parity (unless there is no disparity in recidivism across race to begin with). This impossibility result, discussed in {cite:t}`kleinberg2016inherent` and {cite:t}`chouldechova2017fair`, can be derived fairly straightforwardly using the [Bayes rule](https://en.wikipedia.org/wiki/Bayes%27_theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Proof of the incompatibility between error balance and predictive parity (optional, toggle to show)\n",
    ":class: dropdown\n",
    "Consider a binary classification task with features $X$, sensitive attribute (using gender here as an example) $V \\in \\{F, M \\}$, ground truth $Y \\in \\{0,1\\}$, and classifier prediction $\\widehat{Y} \\in \\{0,1\\}$.\n",
    "\n",
    "The proof will make use of the [Bayes Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), which states that for two events $A$ and $B$\n",
    "\n",
    "$$\n",
    "\\Pr(A|B) = \\frac{\\Pr(B|A)\\Pr(A)}{\\Pr(B)}\n",
    "$$\n",
    "\n",
    "In addition, we need a \"conditioned\" version of the Bayes theorem, where each of the above probability is further conditioned on an event $C$, i.e.,\n",
    "\n",
    "$$\n",
    "\\Pr(A|B, C) = \\frac{\\Pr(B|A, C)\\Pr(A|C)}{\\Pr(B|C)}\n",
    "$$\n",
    "\n",
    "Now, set event $A$ as $Y=1$, event $B$ as $\\widehat{Y}=1$, and event $C$ as $V=F$. By applying the conditional Bayes theorem, we get\n",
    "\n",
    "$$\n",
    "\\Pr(Y=1|\\widehat{Y}=1, V=F) = \\frac{\\Pr(\\widehat{Y}=1|Y=1, V=F)\\Pr(Y=1|V=F)}{\\Pr(\\widehat{Y}=1|V=F)} ~~~~~ Eq(1)\n",
    "$$\n",
    "\n",
    "Similarly, by setting event $C$ as $V=M$, we can obtain another equality\n",
    "\n",
    "$$\n",
    "\\Pr(Y=1|\\widehat{Y}=1, V=M) = \\frac{\\Pr(\\widehat{Y}=1|Y=1, V=M)\\Pr(Y=1|V=M)}{\\Pr(\\widehat{Y}=1|V=M)}~~~~~ Eq(2)\n",
    "$$\n",
    "\n",
    "Notice that: \n",
    "\n",
    "- Error balance on class 1 dictates that $\\Pr(\\widehat{Y}=1|Y=1, V=F) = \\Pr(\\widehat{Y}=1|Y=1, V=M)$;\n",
    "- Predictive parity on class 0 dictates that $\\Pr(Y=1|\\widehat{Y}=1, V=F) = \\Pr(Y=1|\\widehat{Y}=1, V=M)$.\n",
    "\n",
    "Comparing the left-hand-side and right-hand-side of the Eq(1) and Eq(2), we can see that: having error balance and predictive parity at the same time would imply\n",
    "\n",
    "$$\n",
    "\\frac{\\Pr(Y=1|V=F)}{\\Pr(\\widehat{Y}=1|V=F)} = \\frac{\\Pr(Y=1|V=M)}{\\Pr(\\widehat{Y}=1|V=M)}\n",
    "$$\n",
    "\n",
    "Now, pay attention to the denominators, for each value of $V\\in\\{F,M\\}$, we have\n",
    "\n",
    "$$\n",
    "\\Pr(\\widehat{Y}=1|V) = \\Pr(\\widehat{Y}=1|Y=1,V) \\Pr(Y=1|V) + \\Pr(\\widehat{Y}=1|Y=0,V) \\Pr(Y=0|V)\n",
    "$$\n",
    "\n",
    "Among these terms\n",
    "\n",
    "- Error balance on class 1 dictates that $\\Pr(\\widehat{Y}=1|Y=1, V=F) = \\Pr(\\widehat{Y}=1|Y=1, V=M)$;\n",
    "- Error balance on class 0 dictates that $\\Pr(\\widehat{Y}=1|Y=0, V=F) = \\Pr(\\widehat{Y}=1|Y=0, V=M)$\n",
    "\n",
    "So, putting everything together, the only way that we can having error balance and predictive parity at the same time is\n",
    "\n",
    "$$\n",
    "\\Pr(Y=1|V=F) = \\Pr(Y=1|V=M)\n",
    "$$\n",
    "\n",
    "which implies there is no inequality in outcomes across gender to begin with. Contradiction.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impossibility results like this one is not an exception but rather the norm in algorithmic fairness {cite:p}`fazelpour2020algorithmic`, leading some to question \"Can this kind of algorithm ever be done right?\" {cite:p}`guo2025inside`. This is still an open problem at the forefront of algorithmic decision-making research and practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration within Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extension of predictive parity, calibration within groups represents another fairness metric that relies on predicted probabilities. It refers to a property of classification models, known as **calibration**, which requires the predicted probabilities to be \"accurate\" in the sense that they reflect the actual event rate. For example, suppose a well-calibrated model assigns a predicted probability of 70\\% (of being in positive class) to 10 data points, then you would expect 7 out of those data points to actually belong to the positive class. Calibration is a desierable property because it allows users to intuitively interpret the predicted probabilities as actual event rates. Conversely, predicted probabilities from a poorly calibrated model may systematically overestimate or underestimate the actual event rates. Calibration within groups is a strictly stronger notion of fairness than predictive parity. Calibration within groups implies predictive parity but not vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting the notion of calibration for fairness measurement, we say that a classifier achieves calibration within groups if predictions from the two groups are both well-calibrated. More formally,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Equal Calibration\n",
    ":class: tip\n",
    "$$\n",
    "\\mathbb{E}(Y | p, R = AA) = \\mathbb{E}(Y | p, R = W) = p\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate calibration\n",
    "# we will again use decile_score as the predicted probability of recidivism\n",
    "# we will visualize the percentage of recidivism for each decile score and racial group\n",
    "import matplotlib.pyplot as plt\n",
    "# group by decile score and race, then calculate the average value of two_year_recid\n",
    "calibration = compas.groupby(['decile_score', 'race'])['two_year_recid'].mean().reset_index()\n",
    "X = calibration['decile_score'].unique()\n",
    "Y_AA = calibration[calibration['race'] == \"African-American\"]['two_year_recid']\n",
    "Y_W = calibration[calibration['race'] == \"Caucasian\"]['two_year_recid']\n",
    "# plot average recidivism rate for each decile score and the two groups\n",
    "width = 0.4\n",
    "plt.bar(X, Y_AA, width, label = \"African-American\")\n",
    "plt.bar(X + width, Y_W, width, label = \"Caucasian\")\n",
    "plt.xlabel(\"Decile Score\")\n",
    "plt.ylabel(\"Recidivism Rate\")\n",
    "plt.xticks(X + width / 2, X)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we see that the average recidivism rates across different deciles are somewhat similar between the two groups. Among low-risk deciles, the model seems to underestimate the actual recidivism risks for both groups. Among high-risk deciles, it seems to overestimate the risks. Overall, we do not see a very clear pattern and this is aligned with Northpointe's counter-arguments to ProPublica, i.e., their risk tool is similarly calibrated for both racial groups {cite:p}`dieterich2016compas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Measure for Numeric Prediction / Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides fairness measures for binary classification, we also need to consider fairness measures for numeric prediction / regression tasks, where the target variables take continuous values. A few fairness measures designed for binary classification can be generalized to the case of numeric prediction / regression. For demonstration, we will treat the ```decile_score``` as the numerical prediction values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compas_regression = compas.copy()\n",
    "# treating decile_score as the numerical prediction values, no need to remove any rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Conditional) Demographic Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same notions of demographic parity and conditional demographic parity can be defined by comparing the expected values of the predicted outcome between two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Demographic Parity for Regression\n",
    ":class: tip\n",
    "$$\n",
    "\\mathbb{E}(\\widehat{Y} | R = AA) = \\mathbb{E}(\\widehat{Y} | R = W)\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Conditional Demographic Parity for Regression\n",
    ":class: tip\n",
    "$$\n",
    "\\mathbb{E}(\\widehat{Y} | R = AA, \\boldsymbol{X}) = \\mathbb{E}(\\widehat{Y} | R = W, \\boldsymbol{X})\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting demographic parity can be done by essentially running a regression of $\\widehat{Y}$ on $R$ (and potentially controlling for other observables to evaluate the conditional demographic parity). If $R$ is statistically significantly associated with $\\widehat{Y}$, then there will be a significant difference in the expected $\\widehat{Y}$ between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate conditional demographic parity\n",
    "# running a linear regression of decile_score on race, controlling for age, gender, number of prior offenses, and severity of charge\n",
    "import statsmodels.formula.api as smf\n",
    "model = smf.ols(formula = \"decile_score ~ race + age + sex + priors_count + c_charge_degree\", data = compas_regression).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td colspan=\"1\"><em>Dependent variable: Predicted Decile Score</em></td></tr><tr><td style=\"text-align:left\"></td>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "\n",
       "<tr><td style=\"text-align:left\">Intercept</td><td>7.728<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.113)</td></tr>\n",
       "<tr><td style=\"text-align:left\">age</td><td>-0.105<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.003)</td></tr>\n",
       "<tr><td style=\"text-align:left\">Charge Degree Misdemeanor</td><td>-0.456<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.064)</td></tr>\n",
       "<tr><td style=\"text-align:left\">priors_count</td><td>0.274<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.006)</td></tr>\n",
       "<tr><td style=\"text-align:left\">race Caucasian</td><td>-0.534<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.064)</td></tr>\n",
       "<tr><td style=\"text-align:left\">sex Male</td><td>-0.073<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.076)</td></tr>\n",
       "\n",
       "<td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align: left\">Observations</td><td>5278</td></tr><tr><td style=\"text-align: left\">R<sup>2</sup></td><td>0.421</td></tr><tr><td style=\"text-align: left\">Adjusted R<sup>2</sup></td><td>0.420</td></tr><tr><td style=\"text-align: left\">Residual Std. Error</td><td>2.163 (df=5272)</td></tr><tr><td style=\"text-align: left\">F Statistic</td><td>766.453<sup>***</sup> (df=5; 5272)</td></tr>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td><td colspan=\"1\" style=\"text-align: right\"><sup>*</sup>p&lt;0.1; <sup>**</sup>p&lt;0.05; <sup>***</sup>p&lt;0.01</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stargazer.stargazer import Stargazer\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "sg = Stargazer([model])\n",
    "sg.dependent_variable_name('Predicted Decile Score')\n",
    "sg.rename_covariates({\n",
    "    \"c_charge_degree[T.M]\": \"Charge Degree Misdemeanor\",\n",
    "    \"race[T.Caucasian]\": \"race Caucasian\",\n",
    "    \"sex[T.Male]\": \"sex Male\"\n",
    "})\n",
    "sg.show_model_numbers(False)\n",
    "display(HTML(sg.render_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, even after controlling for several observables, being the Caucasian is still negatively and significantly associated with risk decile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual balance is analogous to the Error Balance measure in binary classification, where fairness is measured by the degree to which the model's prediction errors (which is residual in the case of regression) are balanced across the two groups. Note that we do not have the ground truth value for ```decile_score``` in the dataset. For illustration purpose only, we will use the binary label ```is_redic``` as the ground truth, and use ```decile_score``` / 10 as the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definition: Residual Balance for Regression[^footnote2]\n",
    ":class: tip\n",
    "$$\n",
    "\\mathbb{E}( (\\widehat{Y} - Y)^2 | R = AA) = \\mathbb{E}( (\\widehat{Y} - Y)^2 | R = W)\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average squared residuals for African Americans:  0.2295244094488189\n",
      "Average squared residuals for Whites:  0.2261388492629577\n"
     ]
    }
   ],
   "source": [
    "# get the residuals of the regression and store as a new column\n",
    "compas_regression['residuals'] = compas_regression['decile_score'] / 10 - compas_regression['two_year_recid']\n",
    "# evaluate sqaured residuals for African Americans and Caucasians\n",
    "AA_residuals = compas_regression[compas_regression['race'] == 'African-American']['residuals'] ** 2\n",
    "W_residuals = compas_regression[compas_regression['race'] == 'Caucasian']['residuals'] ** 2\n",
    "# print results\n",
    "print(\"Average squared residuals for African Americans: \", AA_residuals.mean())\n",
    "print(\"Average squared residuals for Whites: \", W_residuals.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that African American defendents had slighter greater residuals (indicating less accurate predictions) than White defendents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^footnote1]: Please note that this is different from ProPublica's processing of the data; they pooled \"Medium\" and \"High\" together as \"High\" category.\n",
    "[^footnote2]: Note that one can use other aggregation functions than $L_2$ here, such as absolute difference (i.e., $L_1$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "responsibleai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
