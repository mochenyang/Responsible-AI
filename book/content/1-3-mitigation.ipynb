{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Algorithmic Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter demonstrates a few representative algorithms for mitigating algorithmic bias. As discussed in the Chapter {doc}`1-1-intro`, algorithmic bias can arise from (i) pre-existing bias in data, (ii) bias introduced during model training, and (iii) bias introduced when making predictions / decisions. Accordingly, to mitigate these biases, there are at least three types of approaches:\n",
    "\n",
    "- **Pre-processing Approaches**: pre-process training data to remove existing bias, before training models;\n",
    "- **In-processing Approaches**: modify how models are trained to impose fairness as a learning objective or constraint;\n",
    "- **Post-processing Approaches**: post-process model outputs (e.g., predictions or predicted probabilities) to satisfy certain fairness objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [LSAC Bar Passage Data](https://eric.ed.gov/?id=ED469370) for illustration. This data, originally collected by {cite:t}`wightman1998lsac`, contains the bar passage outcomes and demographic information of over 20,000 individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Data: Compas Recidivism Dataset\n",
    ":class: note\n",
    "- Location: \"data/bar_pass_prediction.csv\"\n",
    "- Shape: (22407, 5)\n",
    "- Note: original dataset has a few more columns. They have been removed for cleaner demonstration.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ```pass_bar``` as the outcome variable of interest, and treat ```race``` as the sensitive feature. We will focus only on white and black races, and remove any rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lsat</th>\n",
       "      <th>ugpa</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>pass_bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lsat  ugpa  gender   race  pass_bar\n",
       "0  44.0   3.5  female  white         1\n",
       "1  29.0   3.5  female  white         1\n",
       "2  36.0   3.5    male  white         1\n",
       "3  39.0   3.5    male  white         1\n",
       "4  48.0   3.5    male  white         1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bar = pd.read_csv('../data/bar_pass_prediction.csv')\n",
    "bar = bar[bar['race'].isin(['white', 'black'])]\n",
    "bar.dropna(inplace = True)\n",
    "bar.reset_index(drop = True, inplace = True)\n",
    "bar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lsat', 'ugpa', 'gender_male', 'race_white'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use lsat, upga, gender, and race as the features\n",
    "X = bar[['lsat', 'ugpa', 'gender', 'race']]\n",
    "Y = bar['pass_bar']\n",
    "# many ML algorithms take numerical input, so let's convert the categorical variables to numerical\n",
    "X = pd.get_dummies(X, columns = ['gender', 'race'], drop_first = True, dtype=int)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first build a baseline classifier for demonstration\n",
    "# using random forest here, please feel free to try other techniques\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# we will use 70% of the data for training and 30% for testing\n",
    "# setting random_state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# train the random forest classifier\n",
    "# this dataset is quite imbalanced, so we will set class_weight to balanced\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughout this chapter, we will evaluate multiple models in terms of both predictive performance and fairness\n",
    "# For predictive performance: we will report the accuracy, precision, recall, and F1 score\n",
    "# note that we set pos_label = 0 because class 0 (not passing the bar) is the minority class in this imbalanced dataset\n",
    "# For fairness: we will report demographic disparity and equalized odds disparity\n",
    "# let's create a function so that we don't need to repeat the same code multiple times\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from fairlearn import metrics\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, pos_label = 0, average = 'binary')\n",
    "    DD = metrics.demographic_parity_difference(y_test, y_pred, sensitive_features = X_test['race_white'])\n",
    "    EOD = metrics.equalized_odds_difference(y_test, y_pred, sensitive_features = X_test['race_white'])\n",
    "    # print all metrics\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 Score:', f1)\n",
    "    print('Demographic Disparity:', DD)\n",
    "    print('Equalized Odds Disparity:', EOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.844939338540801\n",
      "Precision: 0.07529722589167767\n",
      "Recall: 0.19655172413793104\n",
      "F1 Score: 0.10888252148997135\n",
      "Demographic Disparity: 0.16762272165217595\n",
      "Equalized Odds Disparity: 0.21173469387755106\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach: Remove Sensitive Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of pre-processing is to modify the data used for model training to remove the existing bias. Perhaps a seemingly obvious pre-processing approach is to simply drop the sensitive group attribute (```race``` in this case). After all, if the model is \"blind\" to race, it cannot have racial bias, right? Well, let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8311450889147416\n",
      "Precision: 0.07790697674418605\n",
      "Recall: 0.23103448275862068\n",
      "F1 Score: 0.11652173913043479\n",
      "Demographic Disparity: 0.21030078689322695\n",
      "Equalized Odds Disparity: 0.21967584303597842\n"
     ]
    }
   ],
   "source": [
    "# now build another classifier without the race column\n",
    "X_norace_train = X_train.drop(columns = ['race_white'])\n",
    "X_norace_test = X_test.drop(columns = ['race_white'])\n",
    "# train the random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "rf_clf.fit(X_norace_train, y_train)\n",
    "# make predictions on the testing data\n",
    "y_pred_norace = rf_clf.predict(X_norace_test)\n",
    "# evaluate the model\n",
    "evaluate_model(y_test, y_pred_norace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, both demographic disparity and equalized odds disparity actually become greater. In general, removing sensitive feature from data has limited effectiveness. This is because other legitimate features in the data can be correlated with the sensitive feature. Indeed, as shown below, LSAT score and undergraduate GPA are both correlated with race to some degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lsat           0.376971\n",
       "ugpa           0.222320\n",
       "gender_male    0.099201\n",
       "race_white     1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corrwith(X['race_white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "One could argue that the correlations between LSAT score / undergraduate GPA with race are themselves manifestations of existing racial bias in the education system (e.g., perhaps black students systematically received less support in schools, leading to lower LSAT scores and undergraduate GPAs). In generally, what counts as \"legitimate\" or \"non-sensitive\" features can be a point of debate. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this issue, we need to systematically remove the correlations between each non-sensitive feature and the sensitive feature. This can be done via the ```CorrelationRemover``` function in the ```fairlearn``` package. Under the hood, it removes correlations by running linear regressions of non-sensitive features on the sensitive feature and obtaining the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lsat           2.069709e-14\n",
       "ugpa           1.209489e-14\n",
       "gender_male    5.147216e-15\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"race_white\"])\n",
    "X_cr_train = cr.fit_transform(X_train)\n",
    "# transformation returns a numpy array, let's convert it back to a pandas dataframe\n",
    "X_cr_train = pd.DataFrame(X_cr_train, columns = ['lsat', 'ugpa', 'gender_male'])\n",
    "# check correlations again - they are very close to 0 now\n",
    "race_train = X_train['race_white']\n",
    "race_train.reset_index(drop = True, inplace = True)\n",
    "X_cr_train.corrwith(race_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9017782948313113\n",
      "Precision: 0.09433962264150944\n",
      "Recall: 0.1206896551724138\n",
      "F1 Score: 0.1059001512859304\n",
      "Demographic Disparity: 0.061266896331535925\n",
      "Equalized Odds Disparity: 0.07971938775510201\n"
     ]
    }
   ],
   "source": [
    "# now build another classifier with the transformed data\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "rf_clf.fit(X_cr_train, y_train)\n",
    "# make predictions on the testing data\n",
    "y_pred_cr = rf_clf.predict(X_norace_test)\n",
    "# evaluate the model\n",
    "evaluate_model(y_test, y_pred_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both disparities are further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Why does a linear regression remove correlation? (optional, toggle to show)\n",
    ":class: dropdown\n",
    "Given a sensitive attribute $V$ and a non-sensitive attribute $X$ that is correlated with $V$, the correlation remover runs a regression of $X$ on $V$ and collect the residual $X_r$ as the transformed non-sensitive attribute that will be uncorrelated with $V$; that is $Cov(X_r, V) = 0$. This is a general procedure that is effective regardless of the distribution of $X$ or $V$.\n",
    "\n",
    "Intuitively, it works because a linear regression of $X$ on $V$ is trying to use $V$ to account for variations in $X$, and the residual is the variation in $X$ that cannot be accounted for by $V$ (hence independent from $V$).\n",
    "\n",
    "It can be proved more formally. See [this article](https://statproofbook.github.io/P/slr-rescorr.html) for more details if you are interested.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Processing Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to data pre-processing, in-processing approaches aim to mitigate bias by modifying how the model is trained. Many modern machine learning models are trained as an _optimization problem_, i.e., by minimizing a certain loss function (computed over training data). Therefore, some natural ways to mitigate bias include (1) modifying the objective function to have fairness as a part, such as fair regularization approaches ({cite:t}`kamishima2011fairness`); and (2) add fairness as constraints in the optimization problem ({cite:t}`zafar2017fairnessconstraints,cotter2018training,komiyama2018nonconvex,celis2019classification`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Example: fairness regularization\n",
    ":class: tip\n",
    "Consider a resume screening task where a classification model predicts whether candidate $i$ is a good fit for a position or not. Let $Y_i$ denote the ground truth, $\\widehat{Y}_i$ denote the classifier's prediction. Let sets $N_F$ and $N_M$ respectively contain the indices of female and male candidates.\n",
    "\n",
    "The regular classification task typically uses a cross-entropy loss $\\sum_{i=1}^N L(Y_i, \\widehat{Y}_i)$. With the additional consideration of demographic parity, we want the following difference to be small\n",
    "\n",
    "$$\n",
    "\\left | \\frac{1}{|N_F|}\\sum_{i \\in N_F} \\widehat{Y}_i - \\frac{1}{|N_M|}\\sum_{i \\in N_M} \\widehat{Y}_i\\right |\n",
    "$$\n",
    "\n",
    "The new fairness-aware learning task can be formulated as minimizing the following fairness regularized loss function:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N L(Y_i, \\widehat{Y}_i) + \\lambda \\left | \\frac{1}{|N_F|}\\sum_{i \\in N_F} \\widehat{Y}_i - \\frac{1}{|N_M|}\\sum_{i \\in N_M} \\widehat{Y}_i\\right |\n",
    "$$\n",
    "\n",
    "where parameter $\\lambda$ controls the relative importance of achiving demographic parity as compared to achieving greater predictive accuracy.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we will use the ```ExponentiatedGradient``` method, proposed by {cite:t}`agarwal2018reductions` and available within the ```fairlearn``` package. This method reframes a binary classification problem as a constrained optimization problem, with fairness objective(s) set as constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yang3653\\AppData\\Local\\anaconda3\\envs\\responsibleai\\Lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:214: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self.pos_basis[i][\"+\", e, g] = 1\n",
      "c:\\Users\\yang3653\\AppData\\Local\\anaconda3\\envs\\responsibleai\\Lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:215: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self.neg_basis[i][\"-\", e, g] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815855077281037\n",
      "Precision: 0.05735930735930736\n",
      "Recall: 0.18275862068965518\n",
      "F1 Score: 0.08731466227347612\n",
      "Demographic Disparity: 0.026640772040859906\n",
      "Equalized Odds Disparity: 0.0398360044995677\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "fair_obj = DemographicParity()\n",
    "EG_Demo = ExponentiatedGradient(rf_clf, constraints = fair_obj)\n",
    "EG_Demo.fit(X_train, y_train, sensitive_features = X_train['race_white'])\n",
    "y_pred_eg = EG_Demo.predict(X_test, random_state = 42)\n",
    "evaluate_model(y_test, y_pred_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yang3653\\AppData\\Local\\anaconda3\\envs\\responsibleai\\Lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:214: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self.pos_basis[i][\"+\", e, g] = 1\n",
      "c:\\Users\\yang3653\\AppData\\Local\\anaconda3\\envs\\responsibleai\\Lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:215: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self.neg_basis[i][\"-\", e, g] = 1\n",
      "c:\\Users\\yang3653\\AppData\\Local\\anaconda3\\envs\\responsibleai\\Lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:214: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self.pos_basis[i][\"+\", e, g] = 1\n",
      "c:\\Users\\yang3653\\AppData\\Local\\anaconda3\\envs\\responsibleai\\Lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:215: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  self.neg_basis[i][\"-\", e, g] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7915904936014625\n",
      "Precision: 0.05941499085923217\n",
      "Recall: 0.22413793103448276\n",
      "F1 Score: 0.09393063583815028\n",
      "Demographic Disparity: 0.043901797983112\n",
      "Equalized Odds Disparity: 0.06218112244897955\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "fair_obj = EqualizedOdds()\n",
    "EG_EqualOdds = ExponentiatedGradient(rf_clf, constraints = fair_obj)\n",
    "EG_EqualOdds.fit(X_train, y_train, sensitive_features = X_train['race_white'])\n",
    "y_pred_eg = EG_EqualOdds.predict(X_test)\n",
    "evaluate_model(y_test, y_pred_eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, post-processing approaches mitigate bias by changing how a model's predictions are used. For binary classification, one of the most common approaches here is to adjust the prediction threshold. For example, a threshold optimizing approach ({e.g., cite:t}`hardt2016equality`) searches for group-specific thresholds that achieves certain fairness goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9511384410835965\n",
      "Precision: 0.16666666666666666\n",
      "Recall: 0.0034482758620689655\n",
      "F1 Score: 0.006756756756756757\n",
      "Demographic Disparity: 0.001068947087119243\n",
      "Equalized Odds Disparity: 0.00520833333333337\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "# initialize the threshold optimizer\n",
    "# \"constraint\" specifies what kind of fairness goal you want to achieve\n",
    "# \"objective\" specifies the learning objective\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "TO_Demo = ThresholdOptimizer(estimator = rf_clf, constraints = 'demographic_parity', objective = \"accuracy_score\", predict_method = 'auto')\n",
    "TO_Demo.fit(X_train, y_train, sensitive_features = X_train['race_white'])\n",
    "y_pred_to = TO_Demo.predict(X_test, sensitive_features = X_test['race_white'], random_state = 42)\n",
    "evaluate_model(y_test, y_pred_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9511384410835965\n",
      "Precision: 0.16666666666666666\n",
      "Recall: 0.0034482758620689655\n",
      "F1 Score: 0.006756756756756757\n",
      "Demographic Disparity: 0.001068947087119243\n",
      "Equalized Odds Disparity: 0.00520833333333337\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "TO_EqualOdds = ThresholdOptimizer(estimator = rf_clf, constraints = 'equalized_odds', objective = \"accuracy_score\", predict_method = 'auto')\n",
    "TO_EqualOdds.fit(X_train, y_train, sensitive_features = X_train['race_white'])\n",
    "y_pred_to = TO_EqualOdds.predict(X_test, sensitive_features = X_test['race_white'], random_state = 42)\n",
    "evaluate_model(y_test, y_pred_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in this case, the threshold optimizer clearly reduces demographic disparity and disparity in equalized odds. Coincidentally, the results are identical under two constraints. However, this is not always the case (e.g., try replacing the random forest classifier with a gradient boosting classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness-Performance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by tabulating the performance, both predictive and fairness-related, of all the above mitigation approaches. The following table summarizes the (1) accuracy score (as an overall predictive performance measure), (2) F-1 of minority class (as a class-specific performance measure), and (3) demographic disparity and equalized odds disparity (as fairness measures). \"Baseline\" refers to classifier's performance without considering fairness at all; \"DD\" denotes demographic disparity, and \"EOD\" denotes equalized odds disparity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                              | Accuracy | F-1   | DD    | EOD   |\n",
    "| ---------------------------- | -------- | ----- | ----- | ----- |\n",
    "| Baseline                     | 0.845    | 0.109 | 0.168 | 0.212 |\n",
    "| Drop Race                    | 0.831    | 0.117 | 0.210 | 0.220 |\n",
    "| Correlation Remover          | 0.902    | 0.121 | 0.061 | 0.080 |\n",
    "| Exponentiated Gradient (DD)  | 0.816    | 0.087 | 0.027 | 0.040 |\n",
    "| Exponentiated Gradient (EOD) | 0.792    | 0.094 | 0.044 | 0.062 |\n",
    "| Threshold Optimizer (DD)     | 0.951    | 0.007 | 0.001 | 0.005 |\n",
    "| Threshold Optimizer (EOD)    | 0.951    | 0.007 | 0.001 | 0.005 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several observations are worth noting:\n",
    "- There is a tradeoff between predictive performance and fairness performance. In-processing approaches produce more fair predictions than pre-processing approach (correlation remover to be specific) at the cost of having lower accuracy and F-1 scores.\n",
    "- Post-processing approach (i.e., threshold optimizer) results in extremely small fairness disparities, but the F-1 scores are also very low and the accuracy is very high. Considering the imbalanced nature of the dataset, this is not good news. It indicates that the post-processed predictions are almost always class 1 (e.g., if you predict everyone passes the bar, of course there is not demographic disparity). Therefore, just because disparity metrics have lower values do not automatically mean the classifier is more \"useful\".\n",
    "- So which model is the best? Well, this depends on user's tolerance for predictive performance and fairness. If predictive performance takes priority, then correlation remover model may be the best because it produces highest F-1 score while also clearly reduces disparities compared to the baseline. However, if fairness takes priority, then exponentiated gradient models may be prefered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Is the fairness-performance tradeoff always a problematic phenomenon? If achiving certain notion of fairness necessarily sacrifices predictive accuracy, is that always a bad thing? The answer to this question can be quite nuanced and depends on the context.\n",
    "\n",
    "For sake of illustration, imagine an \"extreme case\" of direct discrimination, where an employer only hires male candidates and rejects all female candidates. Any fairness-aware classification model that predicts a positive hiring decision for a female candidate would have made a \"mistake\" from the accuracy perspective. However, these \"mistakes\" often represent a corrective force against historical bias and injustice (direct discrimination in this example). This is a type of **corrective justice**.\n",
    "\n",
    "When thinking about the potential tradeoff between fairness and predictive performance, it is important to reflect on how predictive performance is being measured. If predictive performance is measured based on (biased) historical labels, then high performance can be a signal of perpetuating existing biases.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "responsibleai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
