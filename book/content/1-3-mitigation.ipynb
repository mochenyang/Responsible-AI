{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Algorithmic Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter demonstrates a few representative algorithms for mitigating algorithmic bias. As discussed in the Chapter {doc}`1-1-intro`, algorithmic bias can arise from (i) pre-existing bias in data, (ii) bias introduced during model training, and (iii) bias introduced when making predictions / decisions. Accordingly, to mitigate these biases, there are at least three types of approaches:\n",
    "\n",
    "- **Pre-processing Approaches**: pre-process training data to remove existing bias, before training models;\n",
    "- **In-processing Approaches**: modify how models are trained to impose fairness as a learning objective or constraint;\n",
    "- **Post-processing Approaches**: post-process model outputs (e.g., predictions or predicted probabilities) to satisfy certain fairness objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [LSAC Bar Passage Data](https://eric.ed.gov/?id=ED469370) for illustration. This data, originally collected by {cite:t}`wightman1998lsac`, contains the bar passage outcomes and demographic information of over 20,000 individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Data: Compas Recidivism Dataset\n",
    ":class: note\n",
    "- Location: \"data/bar_pass_prediction.csv\"\n",
    "- Shape: (22407, 5)\n",
    "- Note: original dataset has a few more columns. They have been removed for cleaner demonstration.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ```pass_bar``` as the outcome variable of interest, and treat ```race``` as the sensitive feature. We will focus only on white and black races, and remove any rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lsat</th>\n",
       "      <th>ugpa</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>pass_bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lsat  ugpa  gender   race  pass_bar\n",
       "0  44.0   3.5  female  white         1\n",
       "1  29.0   3.5  female  white         1\n",
       "2  36.0   3.5    male  white         1\n",
       "3  39.0   3.5    male  white         1\n",
       "4  48.0   3.5    male  white         1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bar = pd.read_csv('../data/bar_pass_prediction.csv')\n",
    "bar = bar[bar['race'].isin(['white', 'black'])]\n",
    "bar.dropna(inplace = True)\n",
    "bar.reset_index(drop = True, inplace = True)\n",
    "bar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lsat', 'ugpa', 'gender_male', 'race_white'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use lsat, upga, gender, and race as the features\n",
    "X = bar[['lsat', 'ugpa', 'gender', 'race']]\n",
    "Y = bar['pass_bar']\n",
    "# many ML algorithms take numerical input, so let's convert the categorical variables to numerical\n",
    "X = pd.get_dummies(X, columns = ['gender', 'race'], drop_first = True, dtype=int)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first build a baseline classifier for demonstration\n",
    "# using random forest here, please feel free to try other techniques\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# we will use 70% of the data for training and 30% for testing\n",
    "# setting random_state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# train the random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughout this chapter, we will evaluate multiple models in terms of both predictive performance and fairness\n",
    "# For predictive performance: we will report the accuracy, precision, recall, and F1 score\n",
    "# For fairness: we will report demographic disparity and equalized odds disparity\n",
    "# let's create a function so that we don't need to repeat the same code multiple times\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from fairlearn import metrics\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, pos_label = 1, average = 'binary')\n",
    "    DD = metrics.demographic_parity_difference(y_test, y_pred, sensitive_features = X_test['race_white'])\n",
    "    EOD = metrics.equalized_odds_difference(y_test, y_pred, sensitive_features = X_test['race_white'])\n",
    "    # print all metrics\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 Score:', f1)\n",
    "    print('Demographic Disparity:', DD)\n",
    "    print('Equalized Odds Disparity:', EOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.844939338540801\n",
      "Precision: 0.9557034220532319\n",
      "Recall: 0.8777719573947966\n",
      "F1 Score: 0.915081459907163\n",
      "Demographic Disparity: 0.16762272165217595\n",
      "Equalized Odds Disparity: 0.21173469387755106\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach: Remove Sensitive Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of pre-processing is to modify the data used for model training to remove the existing bias. Perhaps a seemingly obvious pre-processing approach is to simply drop the sensitive group attribute (```race``` in this case). After all, if the model is \"blind\" to race, it cannot have racial bias, right? Well, let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8311450889147416\n",
      "Precision: 0.9567578049253442\n",
      "Recall: 0.8615330888772481\n",
      "F1 Score: 0.9066519661889011\n",
      "Demographic Disparity: 0.21030078689322695\n",
      "Equalized Odds Disparity: 0.21967584303597842\n"
     ]
    }
   ],
   "source": [
    "# now build another classifier without the race column\n",
    "X_norace_train = X_train.drop(columns = ['race_white'])\n",
    "X_norace_test = X_test.drop(columns = ['race_white'])\n",
    "# train the random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "rf_clf.fit(X_norace_train, y_train)\n",
    "# make predictions on the testing data\n",
    "y_pred_norace = rf_clf.predict(X_norace_test)\n",
    "# evaluate the model\n",
    "evaluate_model(y_test, y_pred_norace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, both demographic disparity and equalized odds disparity actually become greater. In general, removing sensitive feature from data has limited effectiveness. This is because other legitimate features in the data can be correlated with the sensitive feature. Indeed, as shown below, LSAT score and undergraduate GPA are both correlated with race to some degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lsat           0.376971\n",
       "ugpa           0.222320\n",
       "gender_male    0.099201\n",
       "race_white     1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corrwith(X['race_white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "One could argue that the correlations between LSAT score / undergraduate GPA with race are themselves manifestations of existing racial bias in the education systme (e.g., perhaps black students systematically received less support in schools, leading to lower LSAT scores and undergraduate GPAs). In generally, what counts as \"legitimate\" or \"non-sensitive\" features can be a point of debate. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this issue, we need to systematically remove the correlations between each non-sensitive feature and the sensitive feature. This can be done via the ```CorrelationRemover``` function in the ```fairlearn``` package. Under the hood, it removes correlations by running linear regressions of non-sensitive features on the sensitive feature and obtaining the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lsat           2.069709e-14\n",
       "ugpa           1.209489e-14\n",
       "gender_male    5.147216e-15\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"race_white\"])\n",
    "X_cr_train = cr.fit_transform(X_train)\n",
    "# transformation returns a numpy array, let's convert it back to a pandas dataframe\n",
    "X_cr_train = pd.DataFrame(X_cr_train, columns = ['lsat', 'ugpa', 'gender_male'])\n",
    "# check correlations again - they are very close to 0 now\n",
    "race_train = X_train['race_white']\n",
    "race_train.reset_index(drop = True, inplace = True)\n",
    "X_cr_train.corrwith(race_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9017782948313113\n",
      "Precision: 0.9548352816153028\n",
      "Recall: 0.9413305395495024\n",
      "F1 Score: 0.9480348193088894\n",
      "Demographic Disparity: 0.061266896331535925\n",
      "Equalized Odds Disparity: 0.07971938775510201\n"
     ]
    }
   ],
   "source": [
    "# now build another classifier with the transformed data\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "rf_clf.fit(X_cr_train, y_train)\n",
    "# make predictions on the testing data\n",
    "y_pred_cr = rf_clf.predict(X_norace_test)\n",
    "# evaluate the model\n",
    "evaluate_model(y_test, y_pred_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both disparities are further reduced, especially disparity in terms of equalized odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Processing Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to data pre-processing, in-processing approaches aim to mitigate bias by modifying how the model is trained. Many modern machine learning models are trained as an _optimization problem_, i.e., by minimizing a certain loss function (computed over training data). Therefore, some natural ways to mitigate bias include (1) modifying the objective function to have fairness as a part, such as fair regularization approaches ({cite:t}`kamishima2011fairness`); and (2) add fairness as constraints in the optimization problem ({cite:t}`zafar2017fairnessconstraints,cotter2018training,komiyama2018nonconvex,celis2019classification`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, post-processing approaches mitigate bias by changing how a model's predictions are used. For binary classification, one of the most common approaches here is to adjust the prediction threshold. For example, a threshold optimizing approach ({e.g., cite:t}`hardt2016equality`) searches for group-specific thresholds that achieves certain fairness goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9511384410835965\n",
      "Precision: 0.951921477291632\n",
      "Recall: 0.99912694255282\n",
      "F1 Score: 0.9749531436360538\n",
      "Demographic Disparity: 0.001068947087119243\n",
      "Equalized Odds Disparity: 0.00520833333333337\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "# initialize the threshold optimizer\n",
    "# \"constraint\" specifies what kind of fairness goal you want to achieve\n",
    "# \"objective\" specifies the learning objective\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "TO_demo = ThresholdOptimizer(estimator = rf_clf, constraints = 'demographic_parity', objective = \"accuracy_score\", predict_method = 'auto')\n",
    "TO_demo.fit(X_train, y_train, sensitive_features = X_train['race_white'])\n",
    "y_pred_to = TO_demo.predict(X_test, sensitive_features = X_test['race_white'], random_state = 42)\n",
    "evaluate_model(y_test, y_pred_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9511384410835965\n",
      "Precision: 0.951921477291632\n",
      "Recall: 0.99912694255282\n",
      "F1 Score: 0.9749531436360538\n",
      "Demographic Disparity: 0.001068947087119243\n",
      "Equalized Odds Disparity: 0.00520833333333337\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42, class_weight = 'balanced')\n",
    "TO_EqualOdds = ThresholdOptimizer(estimator = rf_clf, constraints = 'equalized_odds', objective = \"accuracy_score\", predict_method = 'auto')\n",
    "TO_EqualOdds.fit(X_train, y_train, sensitive_features = X_train['race_white'])\n",
    "y_pred_to = TO_EqualOdds.predict(X_test, sensitive_features = X_test['race_white'], random_state = 42)\n",
    "evaluate_model(y_test, y_pred_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in this case, the threshold optimizer clearly reduces demographic disparity and disparity in equalized odds. Coincidentally, the results are identical under two constraints. However, this is not always the case (e.g., try replacing the random forest classifier with a gradient boosting classifier)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
